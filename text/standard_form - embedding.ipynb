{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcda15e-8293-4c70-b290-f7de2a3dd38f",
   "metadata": {},
   "source": [
    "# Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c85af03-102e-4c64-a366-06e9e556c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘dataset/’: File exists\n",
      "--2024-08-24 06:47:17--  https://huggingface.co/datasets/Mireu-Lab/Korean-jeju-dialect-speech/resolve/main/Processing/Text/csv/processing_utteranceList.csv\n",
      "Resolving huggingface.co (huggingface.co)... 13.225.131.93, 13.225.131.6, 13.225.131.94, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.225.131.93|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/97/a6/97a64cdb19efc35b94db6d9f1b5a01a60a88784965e2a9683966f50d01677271/5791ce2a3d2494ac94ce8e4214a248d3325ae09d9e91eecb68d4d3140c0261b8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27processing_utteranceList.csv%3B+filename%3D%22processing_utteranceList.csv%22%3B&response-content-type=text%2Fcsv&Expires=1724741238&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNDc0MTIzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzk3L2E2Lzk3YTY0Y2RiMTllZmMzNWI5NGRiNmQ5ZjFiNWEwMWE2MGE4ODc4NDk2NWUyYTk2ODM5NjZmNTBkMDE2NzcyNzEvNTc5MWNlMmEzZDI0OTRhYzk0Y2U4ZTQyMTRhMjQ4ZDMzMjVhZTA5ZDllOTFlZWNiNjhkNGQzMTQwYzAyNjFiOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=PEYqahoWv6bB8iPpIRUDV2%7EAEFPilnQ7q4Gk9QdZf6jpERiE1y65E5y7jbwChOnEgDuI%7EMPZeSUpN9qcD5Ywur69-r1cKgNaXFgr7dZBYoRjA%7EkRsfq47AZGp7c5P7xwiGHrWv1Ue4tM935dr8DuYMp5p2nFnTDkDDPLLCXuiomu13FgPnClSFU-CBJerB5j2Od7Sc2QJbuOryPPOsvqNVWfD7NNJ5alkTT4oisdZaG79FxLYrL2vD5WrfifzvZRcFne7sUk29UQ2AwDR3U6ChUfOWjH5i1wWj5cszWuMAJlK26IXrIu7jZJMryUXZi7PlBUKk4qChH4d6rXlCUfMw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2024-08-24 06:47:18--  https://cdn-lfs-us-1.huggingface.co/repos/97/a6/97a64cdb19efc35b94db6d9f1b5a01a60a88784965e2a9683966f50d01677271/5791ce2a3d2494ac94ce8e4214a248d3325ae09d9e91eecb68d4d3140c0261b8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27processing_utteranceList.csv%3B+filename%3D%22processing_utteranceList.csv%22%3B&response-content-type=text%2Fcsv&Expires=1724741238&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNDc0MTIzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzk3L2E2Lzk3YTY0Y2RiMTllZmMzNWI5NGRiNmQ5ZjFiNWEwMWE2MGE4ODc4NDk2NWUyYTk2ODM5NjZmNTBkMDE2NzcyNzEvNTc5MWNlMmEzZDI0OTRhYzk0Y2U4ZTQyMTRhMjQ4ZDMzMjVhZTA5ZDllOTFlZWNiNjhkNGQzMTQwYzAyNjFiOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=PEYqahoWv6bB8iPpIRUDV2%7EAEFPilnQ7q4Gk9QdZf6jpERiE1y65E5y7jbwChOnEgDuI%7EMPZeSUpN9qcD5Ywur69-r1cKgNaXFgr7dZBYoRjA%7EkRsfq47AZGp7c5P7xwiGHrWv1Ue4tM935dr8DuYMp5p2nFnTDkDDPLLCXuiomu13FgPnClSFU-CBJerB5j2Od7Sc2QJbuOryPPOsvqNVWfD7NNJ5alkTT4oisdZaG79FxLYrL2vD5WrfifzvZRcFne7sUk29UQ2AwDR3U6ChUfOWjH5i1wWj5cszWuMAJlK26IXrIu7jZJMryUXZi7PlBUKk4qChH4d6rXlCUfMw__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 54.230.61.24, 54.230.61.49, 54.230.61.15, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|54.230.61.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28228160 (27M) [text/csv]\n",
      "Saving to: ‘dataset/utteranceList.csv’\n",
      "\n",
      "utteranceList.csv   100%[===================>]  26.92M  37.7MB/s    in 0.7s    \n",
      "\n",
      "2024-08-24 06:47:18 (37.7 MB/s) - ‘dataset/utteranceList.csv’ saved [28228160/28228160]\n",
      "\n",
      "--2024-08-24 06:47:18--  https://huggingface.co/datasets/Mireu-Lab/Korean-jeju-dialect-speech/resolve/main/Processing/Text/csv/processing_eojeolList.csv\n",
      "Resolving huggingface.co (huggingface.co)... 13.225.131.35, 13.225.131.94, 13.225.131.6, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.225.131.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/97/a6/97a64cdb19efc35b94db6d9f1b5a01a60a88784965e2a9683966f50d01677271/d9a46c73cc2b81c48be88ccc97e69c6ce37321a47dbada49ecf3d0d4728a5162?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27processing_eojeolList.csv%3B+filename%3D%22processing_eojeolList.csv%22%3B&response-content-type=text%2Fcsv&Expires=1724741239&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNDc0MTIzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzk3L2E2Lzk3YTY0Y2RiMTllZmMzNWI5NGRiNmQ5ZjFiNWEwMWE2MGE4ODc4NDk2NWUyYTk2ODM5NjZmNTBkMDE2NzcyNzEvZDlhNDZjNzNjYzJiODFjNDhiZTg4Y2NjOTdlNjljNmNlMzczMjFhNDdkYmFkYTQ5ZWNmM2QwZDQ3MjhhNTE2Mj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=XG-K8Ph4lknVbwCnRv8j2OAW1ppVFckHtysqeZnj9TnG7yyWjWV4PTOYuHTCFDYFbde4BbfmyKHU0DA41NSNc8RGwCLaBthW54pcrrlrCU63drisUdR4RsP6f8IDthy%7EqOPla8pkCx8z1Z56NW2I4KkjTBN5fgFzztSbgXV9Tu-rHT-nCWk723rwgRysVZXn5qU5u6hjhZFxAhjFDuKVoNMsB7z7zQUOEKVxe7qVHZXUE70T-6Nacj3vZqvPmXTG8GKS9Ot2JwgwGHYN7xtm8GwxeAd%7E5jdpalYzxOeGESyhmK2iR4Ictx-azwpRJqG09Yh4qsPkh97XepOv3XTtRw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2024-08-24 06:47:19--  https://cdn-lfs-us-1.huggingface.co/repos/97/a6/97a64cdb19efc35b94db6d9f1b5a01a60a88784965e2a9683966f50d01677271/d9a46c73cc2b81c48be88ccc97e69c6ce37321a47dbada49ecf3d0d4728a5162?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27processing_eojeolList.csv%3B+filename%3D%22processing_eojeolList.csv%22%3B&response-content-type=text%2Fcsv&Expires=1724741239&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNDc0MTIzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzk3L2E2Lzk3YTY0Y2RiMTllZmMzNWI5NGRiNmQ5ZjFiNWEwMWE2MGE4ODc4NDk2NWUyYTk2ODM5NjZmNTBkMDE2NzcyNzEvZDlhNDZjNzNjYzJiODFjNDhiZTg4Y2NjOTdlNjljNmNlMzczMjFhNDdkYmFkYTQ5ZWNmM2QwZDQ3MjhhNTE2Mj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=XG-K8Ph4lknVbwCnRv8j2OAW1ppVFckHtysqeZnj9TnG7yyWjWV4PTOYuHTCFDYFbde4BbfmyKHU0DA41NSNc8RGwCLaBthW54pcrrlrCU63drisUdR4RsP6f8IDthy%7EqOPla8pkCx8z1Z56NW2I4KkjTBN5fgFzztSbgXV9Tu-rHT-nCWk723rwgRysVZXn5qU5u6hjhZFxAhjFDuKVoNMsB7z7zQUOEKVxe7qVHZXUE70T-6Nacj3vZqvPmXTG8GKS9Ot2JwgwGHYN7xtm8GwxeAd%7E5jdpalYzxOeGESyhmK2iR4Ictx-azwpRJqG09Yh4qsPkh97XepOv3XTtRw__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 54.230.61.24, 54.230.61.49, 54.230.61.15, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|54.230.61.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15636407 (15M) [text/csv]\n",
      "Saving to: ‘dataset/eojeolList.csv’\n",
      "\n",
      "eojeolList.csv      100%[===================>]  14.91M  35.8MB/s    in 0.4s    \n",
      "\n",
      "2024-08-24 06:47:19 (35.8 MB/s) - ‘dataset/eojeolList.csv’ saved [15636407/15636407]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sh ../download_TextDataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aad870-8113-4c67-a489-46dc5fd4647c",
   "metadata": {},
   "source": [
    "# Python Lib Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8866e17e-d254-4554-b4c6-52c3b91c5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447f5a07-d1e8-4b1a-9c4d-733795affe49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/site-packages (from tokenizers) (0.24.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d43a41-c7b9-4e24-a35b-ad725a96d26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/site-packages (from transformers) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d857e26-e652-4c27-a30c-8da9739899d4",
   "metadata": {},
   "source": [
    "# BERT Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e2295-3e74-41b7-afbd-e2582ce72fa2",
   "metadata": {},
   "source": [
    "## Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94aef1f7-3c18-4848-ae40-92a6084f62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentenceDataFrame = pd.read_csv(\"dataset/utteranceList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce11821c-5987-4a87-9fd3-47074708ad06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_form</th>\n",
       "      <th>dialect_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 만나서 반가워 아 오늘 제주 방언 에이 아이 데이터</td>\n",
       "      <td>언니 만낭 반가워 아 오늘 제주 방언 에이 아이 데이터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어 그거 대화 주제는 오늘 대화 주제는</td>\n",
       "      <td>어 그거 대화 주제는 오늘 대화 주제는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>어 명절 설 명절 추석 명절 요로케 나누어서 해볼 거예요</td>\n",
       "      <td>어 명절 설 명절 추석 명절 요로케 나누어서 해볼 거예</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>자 그러면 이제부터 얘기해 봐요</td>\n",
       "      <td>자 그믄 이제부터 얘기해 보게예</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>어 그러면 언니네 설 명절 때 음식 어떻게 해?</td>\n",
       "      <td>어 그믄 언니네 설 명절 때 음식 어떵 해?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292044</th>\n",
       "      <td>마무리 해서 저녁에 잠깐 나갈게.</td>\n",
       "      <td>마무리 해그넹 저녁에 잠깐 나가크라.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292045</th>\n",
       "      <td>어 그렇게 해라</td>\n",
       "      <td>어 겅 하라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292046</th>\n",
       "      <td>카페나 가자.</td>\n",
       "      <td>카페나 가게.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292047</th>\n",
       "      <td>난 차 없으니까 우리 집 못가</td>\n",
       "      <td>난 차 어시난 우리 집 못가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292048</th>\n",
       "      <td>그게 목적</td>\n",
       "      <td>그게 목적</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292049 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          standard_form                    dialect_form\n",
       "0       언니 만나서 반가워 아 오늘 제주 방언 에이 아이 데이터  언니 만낭 반가워 아 오늘 제주 방언 에이 아이 데이터\n",
       "1                 어 그거 대화 주제는 오늘 대화 주제는           어 그거 대화 주제는 오늘 대화 주제는\n",
       "2       어 명절 설 명절 추석 명절 요로케 나누어서 해볼 거예요  어 명절 설 명절 추석 명절 요로케 나누어서 해볼 거예\n",
       "3                     자 그러면 이제부터 얘기해 봐요               자 그믄 이제부터 얘기해 보게예\n",
       "4            어 그러면 언니네 설 명절 때 음식 어떻게 해?        어 그믄 언니네 설 명절 때 음식 어떵 해?\n",
       "...                                 ...                             ...\n",
       "292044               마무리 해서 저녁에 잠깐 나갈게.            마무리 해그넹 저녁에 잠깐 나가크라.\n",
       "292045                         어 그렇게 해라                          어 겅 하라\n",
       "292046                          카페나 가자.                         카페나 가게.\n",
       "292047                 난 차 없으니까 우리 집 못가                 난 차 어시난 우리 집 못가\n",
       "292048                            그게 목적                           그게 목적\n",
       "\n",
       "[292049 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c802e4c-c5a3-48ed-90bc-ce0305b95fb7",
   "metadata": {},
   "source": [
    "# Tokenizer Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b382ab-30d1-4a7f-b42a-626632effc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer \n",
    "\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399ce552-0954-4ec8-8933-37e3a849b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertWordPieceTokenizer 선언\n",
    "# None 자리는 원래 vocabulary 파일 경로가 들어가는 자리입니다.\n",
    "# 하지만 저희는 새로운 vocabulary를 만들것이기 때문에 None으로 만듭니다.\n",
    "# 그 외에는 자유롭게 변경하셔도 되는 부분입니다.\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    None,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # Must be False if cased model\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d460717e-a747-4173-9b31-f01dbdb6e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련하기\n",
    "    # .train을 쓰면 파일 경로를 주어서 학습할 수 있고\n",
    "    # .train_from_iterator를 쓰면은 List[str] 형태를 주어 학습할 수 있습니다.\n",
    "    # special token과 vocab size를 정하고 학습합니다.\n",
    "tokenizer.train_from_iterator(\n",
    "    sentenceDataFrame[\"standard_form\"].to_list(),\n",
    "    vocab_size=36000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens = [\"[PAD]\", \"[CLS]\", \"[UNK]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622d0bf-032a-4ccc-9d93-3444a1384046",
   "metadata": {},
   "source": [
    "## Tokenizer Train Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc6c058-72bc-4e57-869b-3b47d9089741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_dir = 'standard_form.txt'\n",
    "\n",
    "# tokenizer에서 vocab 가져오기\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# index 번호에 맞게 정렬\n",
    "vocabulary = [[v, k] for k, v in vocab.items()]\n",
    "vocabulary = sorted(vocabulary)\n",
    "vocabulary = list(np.array(vocabulary)[:, 1])\n",
    "\n",
    "# vocabulary 저장\n",
    "with open(vocab_dir, 'w+') as lf:\n",
    "    lf.write('\\n'.join(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e2b21-7556-472f-a7fa-3a03cc4d0a04",
   "metadata": {},
   "source": [
    "# Predict Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22d5e71-1618-494e-a882-1a18627220bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2165: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer(vocab_dir, do_basic_tokenize=False)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(vocab_dir, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec84a72-cae0-4ab1-bb27-25f0ddd1d641",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06449200-3f4b-40bb-9b34-9713f7ace574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자 그러면 이제부터 얘기해 봐요\n",
      "['자', '그러면', '이제부터', '얘기해', '봐요']\n",
      "[1, 712, 2073, 10778, 3065, 5545, 3]\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "\n",
    "standard_form = sentenceDataFrame[\"standard_form\"].to_list()\n",
    "print(standard_form[index])\n",
    "print(bert_tokenizer.tokenize(standard_form[index]))\n",
    "print(bert_tokenizer.encode(standard_form[index]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
